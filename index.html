<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Required meta tags -->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<!-- Bootstrap CSS -->
<link rel="stylesheet" href="./css/bootstrap.min.css">

<!-- Icons -->
<link rel="stylesheet" href="./css/font-awesome.min.css">
<link rel="stylesheet" href="./css/academicons.min.css">
<link href="css/css2" rel="stylesheet">

<!-- Fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:ital,wght@0,400;0,600;1,400;1,600&display=swap" rel="stylesheet">

<link rel="stylesheet" href="./css/style.css">

<title>Chenglei Si</title>
</head>

<body>
<div class="container pt-3">
<div class="row">
<div class="col-md-8 offset-md-2">

<a href="#about">[about]</a> &nbsp;
<a href="#papers">[papers]</a> &nbsp;
<a href="#travels">[travels]</a> &nbsp;
<a href="#talks">[talks]</a> &nbsp;
</br></br></br>
<!-- 
<div 
  class="image-container" 
  onmouseover="this.style.backgroundImage='url(images/fun_photo.jpg)';"
  onmouseout="this.style.backgroundImage='url(images/photo.jpg)';"
></div> -->
<img src="images/drawing.jpeg" alt="my face" class="img-fluid rounded" width="250" height="250" onmouseover="this.src='images/naacl.jpg';" onmouseout="this.src='images/sequoia.jpeg';">
<!-- <img src="images/photo.jpeg" alt="my face" class="img-fluid rounded" width="250" height="250"> -->
</br>
<!-- (photo credit: <a href="https://www.jordanjuravsky.com/">Jordan</a>) -->
</br></br>
<h3>Chenglei Si <small><span style="font-size: 0.8em">(チェンレイ・シ)</span></small></h3>
<a href="mailto:sichenglei1125@gmail.com">[email]</a>
<a href="https://scholar.google.com.sg/citations?user=CyKr1q8AAAAJ&hl=en">[scholar]</a>
<a href="https://twitter.com/ChengleiSi">[twitter]</a>
<a href="https://github.com/NoviScl">[github]</a>
<!-- </br>Undergrad at UMD <a href="https://wiki.umiacs.umd.edu/clip/index.php/Main_Page">CLIP</a> & <a href="https://languagescience.umd.edu/">LSC</a> -->
</br>2nd Year PhD at <a href="https://nlp.stanford.edu/">Stanford NLP</a> 
</br></br>

<a id="about"><h3>About</h3></a>

At Stanford, I'm advised by the wonderful <a href="https://thashim.github.io/">Tatsu Hashimoto</a> and <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>. 
Before coming to Stanford, I did my undergrad at the University of Maryland where I was advised by <a href="http://users.umiacs.umd.edu/~jbg/">Jordan Boyd-Graber</a>.
I also had the fortune to work with many amazing research mentors and collaborators, including <a href="https://hci.stanford.edu/msb/">Michael Bernstein</a>, <a href="https://users.umiacs.umd.edu/~hal/">Hal Daumé III</a>, <a href="https://hhexiy.github.io/">He He</a>, <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>, <a href="https://www.cs.cmu.edu/~sherryw/">Sherry Wu</a>, <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>, <a href="https://www.comp.nus.edu.sg/~kanmy/">Min-Yen Kan</a>, and many others. 
<!-- while also working closely with 
 <a href="http://users.umiacs.umd.edu/~hal/">Hal Daumé III</a>, <a href="https://hhexiy.github.io/">He He</a>, <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>,
and <a href="https://www.cs.cmu.edu/~sherryw/">Sherry Wu</a>.
In summer 2022, I did a research internship at Microsoft hosted by <a href="https://zhegan27.github.io/">Zhe Gan</a>. Before that, I got into NLP research by working with 
<a href="https://www.comp.nus.edu.sg/~kanmy/research.html">Min-Yen Kan</a> and <a href="http://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>. -->
</br></br>

I'm interested in how LLMs can transform scientific research, and new forms of human-AI co-intelligence. Towards this end, I split my time between working with LLMs and running human studies. 
When not writing papers, I also <a href="https://x.com/ChengleiSi/status/1889142120633766368">write</a> <a href="https://x.com/ChengleiSi/status/1731047065382523332?s=20">research</a> <a href="https://x.com/ChengleiSi/status/1742977971890352362?s=20">related</a> <a href="https://x.com/ChengleiSi/status/1664023767373299715?s=20">threads</a> <a href="https://x.com/ChengleiSi/status/1795568536540377576">for fun</a>.
</br></br>

Research style: I've come to the realization that I'm most productive when focusing on only 1-2 projects every year. And I prefer projects that are more ambitious and risky in nature (which also tend to be longer-term). This means that I have to turn down all the collaboration requests that don't fall into this category -- I'm sorry! 

<!-- Very recently, I have pivoted my research agenda to think about <a href="https://arxiv.org/abs/2311.02462">AGI</a> and <a href="https://arxiv.org/abs/2311.11388">Machine Culture</a>.
As AI researchers, we have seen ample studies on how to adapt AI to align with human values, or complement human capabilities. 
But while humans are re-designing modern AI, the increasingly powerful and ubiquitous AI technologies (GPT-4, Midjourney, AlphaFold) are also producing real artifacts, 
impacting real people, and changing the way we humans think, communicate, and collaborate.
At a macro level, the rise of AGI is profoundly shaping our value, culture, governance, economy, and incentives as a society.  
I am intrigued to design empirical studies to rigorously understand such impact both at the individual level and at the societal level, and to envision the future of human-AGI <a href="https://groups.csail.mit.edu/medg/people/psz/Licklider.html">symbiosis</a>. -->

<!-- 
How would the relationship between humans and AI evolve, as AI becomes increasingly capable?
What is the boundary between automation and augmentation?
What are the capabilities that humans are fundamentally limited at, and what are the things where humans are inherently irreplaceable?
I like to take a slightly aggressive stance on exploiting the power of AI, while grounding all my conclusions in rigorous empirical studies and thinking carefully about the human factors involved. -->

<!-- Nowadays, I'm fascinated by <strong>Human-<a href="https://www.youtube.com/watch?v=u0DgoRVLTE8">Muppet</a> Interaction</strong>.
And I'm particularly concerned about the following questions:

<div style="margin-bottom:0.3cm;margin-top:0.3cm;margin-left:1cm;">
    How can humans verify what Muppets said, especially in tasks where humans lack the domain expertise?</br>
    How can we enable human-Muppet collaboration to complete tasks that humans or Muppets cannot solve alone?</br>
    What is the long-term impact of humans relying on Muppets?</br>
    How can we measure and improve the safety of Muppets?</br>
</div>

In pursuing these research questions, I tend to move away from existing benchmarks and put human needs at the <a href="https://twitter.com/haldaume3/status/1644435917610840067">center</a> of my research.
I also aim to craft an interdisciplinary research agenda that connects insights from HCI, NLP, ML, Psychology, and Linguistics. 
</br></br> -->

<!-- Back in the old days, I also worked on Question Answering, Tokenization, and Prompting. </br> -->


<div style="margin-bottom:0.8cm;"></div>


<a id="papers"><h3>Selected Papers</h3></a>
<ul>

<li>
  <strong>The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas
</strong><br>
  Chenglei Si, Tatsunori Hashimoto, Diyi Yang <br>
  <i>preprint</i>
  <a href="https://arxiv.org/abs/2506.20803">[paper]</a>
  <a href="https://github.com/NoviScl/AI-Researcher">[data]</a>
  <a href="https://x.com/ChengleiSi/status/1939676648418509051">[tweet]</a>
  <br>
  </li><br>

<li>
  <strong>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers
</strong><br>
  Chenglei Si, Diyi Yang, Tatsunori Hashimoto <br>
  <i>ICLR 2025</i>
  <a href="https://arxiv.org/abs/2409.04109">[paper]</a>
  <a href="https://github.com/NoviScl/AI-Researcher">[code]</a>
  <a href="https://x.com/ChengleiSi/status/1833166031134806330">[tweet]</a>
  <a href="https://www.nature.com/articles/d41586-024-03070-5">[Nature News]</a>
  <a href="https://www.newsweek.com/ai-better-humans-crucial-aspect-scientific-research-study-finds-1952896">[Newsweek]</a>
  <a href="https://syncedreview.com/2024/09/17/stanfords-landmark-study-ai-generated-ideas-rated-more-novel-than-expert-concepts/">[Synced]</a>
  <br>
  </li><br>


<li>
  <strong>Predicting Empirical AI Research Outcomes with Language Models
</strong><br>
Jiaxin Wen, Chenglei Si, Yueh-han Chen, He He, Shi Feng
<br>
  <i>preprint</i>
  <a href="https://arxiv.org/abs/2506.00794">[paper]</a>
  <a href="https://x.com/jiaxinwen22/status/1929937189980250505">[tweet]</a>
  <br>
  </li><br>

<!--     
<li>
  <strong>Design2Code: How Far Are We From Automating Front-End Engineering?
</strong><br>
  Chenglei Si*, Yanzhe Zhang*, Zhengyuan Yang, Ruibo Liu, Diyi Yang <br>
  <i>NAACL 2025</i>
  <a href="https://arxiv.org/abs/2403.03163">[paper]</a>
  <a href="https://github.com/NoviScl/Design2Code">[code]</a>
  <a href="https://salt-nlp.github.io/Design2Code/">[project page]</a>
  <a href="https://x.com/ChengleiSi/status/1765790273693790643?s=20">[tweet]</a>
  <a href="https://youtu.be/i90DzCNVkog?si=fh6bgcJ1YuxJBGgm&t=215">[WorldofAI]</a>
  <br>
  </li><br>

<li>
  <strong>Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong
</strong><br>
  Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daumé III, Jordan Boyd-Graber <br>
  <i>NAACL 2024</i>
  <a href="https://arxiv.org/abs/2310.12558">[paper]</a>
  <a href="https://twitter.com/ChengleiSi/status/1717207134944039161">[tweet]</a>
  <br>
  </li><br> -->

<!-- 
<li>
  <strong>Mixture of Prompt Experts for Generalizable and Interpretable Question Answering</strong><br>
  Chenglei Si, Weijia Shi, Chen Zhao, Luke Zettlemoyer, Jordan Boyd-Graber <br>
  <i>EMNLP 2023 Findings</i>
  <a href="https://arxiv.org/abs/2305.14628">[paper]</a>
  <a href="https://github.com/NoviScl/MoPE">[code]</a>
  <br>
  </li><br> -->


<!-- <li>
  <strong>Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition
</strong><br>
Sander Schulhoff*, Jeremy Pinto*, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan Boyd-Graber <br>
  <i>EMNLP 2023</i>
  <a href="https://trigaten.github.io/hackaprompt.github.io/">[webpage]</a>
  <br>
  <strong>Best Theme Paper</strong>
  </li><br> -->

<!-- 
<li>
  <strong>Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations</strong><br>
  Chenglei Si*, Dan Friedman*, Nitish Joshi, Shi Feng, Danqi Chen, He He <br>
  <i>ACL 2023</i>
  <a href="https://arxiv.org/abs/2305.13299">[paper]</a>
  <a href="https://github.com/NoviScl/AmbigPrompt">[code]</a>
  <a href="https://twitter.com/ChengleiSi/status/1661772241292386304">[tweet]</a>
  <a href="https://openreview.net/forum?id=BcbwGQWB-Kd">[OpenReview]</a>
  <br>
  </li><br>
  

<li>
<strong>Prompting GPT-3 To Be Reliable</strong><br>
Chenglei Si,
Zhe Gan,
Zhengyuan Yang,
Shuohang Wang,
Jianfeng Wang,
Jordan Boyd-Graber,
Lijuan Wang <br>
<i>ICLR 2023</i>
<a href="https://arxiv.org/abs/2210.09150">[paper]</a>
<a href="https://github.com/NoviScl/GPT3-Reliability">[code]</a>
<a href="https://twitter.com/ChengleiSi/status/1582473478543794177?s=20&t=AH7vbRJ9za77qJo9dVvSqg">[tweet]</a>
<a href="https://youtu.be/bgBCSIGOYw4">[video]</a>
<br>
</li><br> -->

<!-- <li>
  <strong>Sub-Character Tokenization for Chinese Pretrained Language Models</strong><br>
  Chenglei Si*,
  Zhengyan Zhang*,
  Yingfa Chen*,
  Fanchao Qi,
  Xiaozhi Wang,
  Zhiyuan Liu, 
  Yasheng Wang,
  Qun Liu,
  Maosong Sun <br>
  <i>TACL 2023</i>
  <a href="https://arxiv.org/abs/2106.00400">[paper]</a>
  <a href="https://github.com/NoviScl/WubiBERT">[code]</a>
  <br>
  </li><br>

  <li>
    <strong>Re-Examining Calibration: The Case of Question Answering</strong><br>
    Chenglei Si,
    Chen Zhao,
    Sewon Min,
    Jordan Boyd-Graber <br>
    <i>EMNLP 2022 Findings</i>
    <a href="https://arxiv.org/abs/2205.12507">[paper]</a>
    <a href="https://github.com/NoviScl/calibrateQA">[code]</a>
    <a href="https://www.youtube.com/watch?v=gbwJcEgczeA">[video]</a>
    <br>
  </li> -->

</ul>
<div style="margin-bottom:0.8cm;"></div>


<a id="talks"><h3>Talks</h3></a>
<ul>
  <li>
    <strong>Fall 2024: The Dream of Automating Research</strong><br>
    Stanford, USC, MIT, Northeastern, MILA, Intel, Samaya AI <br>
    <a href="https://www.figma.com/slides/2bfB5nb498XVCNQd6aJCjU/AutomatingResearchPublic?node-id=1-75&t=pyqaIV9qSyaLFviG-1">[slides]</a>
    <a href="https://www.youtube.com/watch?v=IOCi6q3KvX4">[podcast]</a>
    <br>
    </li><br>

    <li>
      <strong>Summer 2025: The Art of Evaluating Research Ideas</strong><br>
      UVA,  alphaXiv, Ai2, CMU<br>
      <a href="https://www.figma.com/slides/DLQFsO3nCkABs8b8kDtYtt/alphaXiv?t=tHRW6pnraZkbD1Dc-1">[slides (v1)]</a>
      <a href="https://www.figma.com/slides/Cd1ZWpPx2Zc2NnL9szpuim/Ai2-Talk?node-id=1-536&t=sSNOC5WCX5Rhogyy-1">[slides (v2)]</a>
      <br>
      </li><br>
</ul>
</br>

<a id="travels"><h3>Conference Travels</h3></a>
<ul>
    <li><strong>April 2025,</strong> ICLR @ Singapore</li>
    <li><strong>Dec 2024,</strong> NeurIPS @ Vancouver</li>
    <li><strong>June 2024,</strong> NAACL @ Mexico City</li>
    <li><strong>Oct 2023,</strong> UIST @ San Francisco</li>
    <li><strong>July 2023,</strong> ACL @ Toronto</li>
    <li><strong>May 2023,</strong> ICLR @ Kigali</li>
    <li><strong>Dec 2022,</strong> EMNLP @ Abu Dhabi</li>
    <li><strong>July 2022,</strong> NAACL @ Seattle</li>
</ul>
</br>

<div style="margin-bottom:0.8cm;"></div>
</div>
</div>
</div>
</div>
</body>

</html>
